---
title: "Ayudantía 4 - ML para Negocios"
date: "7 de septiembre del 2022"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "light"
    downcute_theme: "default"
---

```{=html}
<style type="text/css">
body{
  font-size: 15pt;
}
.Wrap {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

```{r setup, include=FALSE, message=FALSE}
library(here)


## Global options
knitr::opts_chunk$set(cache = TRUE)

# Here
here::i_am('Ayudantía 4/Ayudantía 4.Rmd')
```

# Introducción

Bienvenid\@s a la cuarta ayudantía de EAA3707 - Machine Learning para Negocios. En la ayudantía veremos:

1.  Recordatorio modelo de regresión logística
2.  Supuestos del modelo de regresión logística
    1. Linealidad del logit
    2. Independencia de las observaciones
    3. Test de Hosmer & Lemeshow
    4. Colinealidad de los predictores
3. Discusión: Regresión logística altamente no balanceada 
4. Introducción KNN

Antes de comenzar, cargamos las librerías que utilizaremos durante la ayudantía.

```{r librerias, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)


# Para obtener resultados reproducibles
set.seed(912)
```

Además, les recuerdo los libros que les recomiendo para aprender de R:

-   **Hands-On Programming with R**: disponible [acá](https://rstudio-education.github.io/hopr/). Útil para recordar cosas básicas de R.

-   **Advanced R**: disponible [acá](https://adv-r.hadley.nz/). Para aprender R avanzado (realmente avanzado), si es que están interesados en entender cómo funciona R por detrás.

-   **R for Data Science**: disponible [acá](https://r4ds.had.co.nz/). Bueno para aprender y aplicar Data Science de manera rápida, además de ser uno de los libros guía para aprender acerca de los paquetes de tidyverse.

-   **RMarkdown Cookbook**: disponible [acá](https://bookdown.org/yihui/rmarkdown-cookbook/). Útil para aprender lo básico de RMarkdown.

-   **RMarkdown: The Definitive Guide**: disponible [acá](https://bookdown.org/yihui/rmarkdown/). Útil para aprender cosas avanzadas de RMarkdown.

-   **Tidy Modeling with R**: disponible [acá](https://www.tmwr.org/). Recomendado para aprender del ecosistema tidymodels. Tiene también un capítulo pequeño de tidyverse con lo justo y necesario.

# Recuerdo del modelo de regresión logística

<center>![Modelo de regresión logística](https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning.png)</center>

Antes de partir con la ayudantía, es importante recordar los conceptos y definiciones importantes del modelo de regresión logística.

Recordemos que en el caso de regresión logística el modelo puede ser escrito como:

$$
\begin{align}
y_i|\mathbf{x}_i &\overset{ind}{\sim} \text{Bernoulli}(\theta_i) \\
g(\theta_i) &= \beta_0 + \beta_1x_{i, 1} + ... + \beta_p x_{i, p} = x^T \beta
\end{align}
$$

donde $g$ es la función logit, esto es,

$$
g(\theta_i) = \text{logit }\theta_i = \log \frac{\theta_i}{1 - \theta_i}
$$

En palabras, lo anterior nos dice que proponemos un modelo en el cual se modela la variable respuesta, $Y$, como realizaciones de una variable aleatoria Bernoulli, esto es, $Y=1$ con probabilidad $\theta$ o $Y=0$ con probabilidad $1 - \theta$, donde el **logit de la media** de la distribución se modela como una función lineal de ciertos predictores o covariables.

--
**Nota**: recuerde que si $Y \sim \text{Bernoulli}(p)$ entonces $E(Y) = p$, por lo que el parámetro corresponde justamente a la media. Esto no necesariamente sucede con cualquier modelo.
--

De manera equivalente, notemos que

$$
g(\theta_i) = \log \frac{\theta_i}{1 - \theta_i} = x_i^T \beta\Leftrightarrow \theta_i = \frac{\exp(x^T\beta)}{1 + \exp(x^T\beta)}
$$

por lo tanto, también podemos decir que la **media** de la distribución es la función logística de la componente lineal $x^T\beta$ (a veces denominada componente sistemática)

--
**Nota**: Como se mencionó la semana pasada, lo único que necesitamos de la función $g$ es que lleve valores de $(0, 1)$ a $(-\infty, \infty)$. La función logit es solo una de ellas, pero está también por ejemplo la función probit o la log-log-complementaria (de hecho, si se fijan la inversa de cualquier función de distribución acumulada cumple lo pedido). Pese a esto, en la práctica el modelo se sigue denominando regresión logística, solo que decimos que cambiamos la **función de enlace**.
--

En la figura a continuación podemos ver las diferencias entre los enlaces logit, probit y log-log-complementario.

```{r distintos enlaces, echo=FALSE, fig.align='center'}
logistic <- function(x){exp(x)/(exp(x) + 1)}
probit <- function(x){pnorm(x)}
llc <- function(x){1 - exp(-exp(x))}

ggplot(data.frame(x = c(-10, 10)), aes(x = x)) + 
  stat_function(fun = logistic, col = 'red', lwd = 1) +
  stat_function(fun = probit, col = 'blue', lwd = 1) +
  stat_function(fun = llc, col = 'green', lwd = 1) +
  geom_hline(yintercept = c(0, 1), lty = 'dashed') +
  geom_hline(yintercept = 0.5, lty = 'dashed') +
  geom_vline(xintercept = 0, lty = 'dashed') +
  ylim(-0.1, 1.1) +
    labs(title = 'Diferentes funciones de enlace',
         subtitle = 'Rojo - logit, Azul - probit, Verde - llc',
         y = 'p(Y=1|x)')
```

Notamos de lo anterior que tanto el logit como el probit son funciones de enlace simétricas, pero no así el enlace log-log-complementario.

## Definiciones importantes

También es importante recordar la siguiente terminología, que es muy utilizada en el contexto de regresión logística.

Las **chances** u **odds** de un suceso es el cociente entre sus probabilidades de ocurrencia y sus probabilidades de no ocurrencia, esto es,

$$
\text{Odds}(A) = \frac{P(A)}{1 - P(A)}
$$

En el caso de regresión logística con $p$ predictores, el odds de un suceso es el cociente entre sus probabilidades de ocurrencia y sus probabilidades de no ocurrencia, dejando $x$ fijo, esto es

$$
\text{Odds}(Y=1|x) = \frac{\theta(x)}{1 - \theta(x)} = \exp(x^T\beta)
$$

El cambio proporcional en los odds se calcula dividiendo los odds después de un cambio y los odds antes del cambio, lo que se conoce como la **razón de chances** u **odds ratio**.

En el caso de regresión logística, tenemos que si el predictor $j$ aumenta una unidad, entonces

$$
\text{OR}(x_j + 1, x_j) = \frac{\text{Odds}(Y=1|x_j + 1)}{\text{Odds}(Y = 1|x_j)} = \exp(\beta_j)
$$

luego, $\exp(\beta_j)$ se interpreta como el factor por el que cambian las chances cuando el predictor $j$, $x_j$, aumenta en una unidad.

# Supuestos del modelo de regresión logística

Cada modelo que proponemos para describir la realidad se basan, algunos en menor medida, en supuestos que hacemos para simplificar el análisis, sin una gran pérdida en generalidad.

En particular, el modelo de regresión logística también se basa en ciertos supuestos, pero para entenderlos mejor es más fácil ver los supuestos del modelo de regresión que ya han visto.

--
**Nota**: es probable que ya vieron el tema de los supuestos en su curso de estadística, así que pasaré muy por encima en este caso.
--

## Supuestos modelo de regresión lineal




# Ajuste de un modelo de regresión logística

## Datos a utilizar

Pasemos ahora a la parte práctica. La base de datos `credit.data` contiene información crediticia de diferentes clientes de un banco alemán. En particular, la base de datos tiene información de 1000 clientes, con sus diferentes atributos, así como una variable que indica si la persona tiene un buen o mal riesgo crediticio.

Carguemos en primer lugar la base de datos y echemos un vistazo rápido a los datos.

```{r carga datos, message=FALSE}
# Cargamos los datos (usaremos RStudio directamente)
credit_full <- readr::read_table(
  file = here::here('Ayudantía 3', 'german.data'), 
  col_names = FALSE
)

# Echamos un vistazo a los datos
dplyr::glimpse(credit_full)
```

A partir de lo anterior notamos que la base de datos viene sin nombres de las columnas, y que los valores vienen codificados en un formato que no sabemos a qué se refieren. En estos casos lo mejor es ver la fuente de los datos, que en este caso los obtuve del [Machine Learning Repository](https://archive-beta.ics.uci.edu/ml/datasets/statlog+german+credit+data) de la University of California, Irvine. De ahí obtenemos que las variables corresponden a, por ejemplo, el estado actual de la cuenta corriente, la duración en meses del crédito y el propósito del crédito.

Para simplificar el análisis solo utilizaremos los atributos 2, 3, 5, 13, 15, 16 y obviamente nuestra variable respuesta. En el código a continuación realizamos esta selección, colocamos nombres explicativos y cambiamos las variables cualitativas a factor. Cambiaré también la codificación de `housing`, pero solo para mostrar cómo se hace.

```{r selección y nombres}
credit <- 
  credit_full %>% 
  dplyr::select(X2, X3, X5, X13, X15, X16, X21) %>% 
  dplyr::rename(duration = X2,
                credit_history = X3,
                credit_amount = X5,
                age = X13,
                housing = X15,
                n_credits = X16,
                risk = X21) %>% 
  dplyr::mutate(housing = dplyr::recode(housing, 
                                        "A151" = 'rent',
                                        "A152" = 'own',
                                        "A153" = 'free'),
                risk = dplyr::recode(risk,
                                     `1` = 'good',
                                     `2` = 'bad')) %>% 
  dplyr::mutate(across(where(is.character), as_factor))

# Veamos nuevamente los datos pero ahora arreglados
dplyr::glimpse(credit)
```

Ya teniendo nuestros datos listos, realizamos un pequeño análisis exploratorio.

## Análisis exploratorio

En primer lugar, nos interesa saber la proporción de clientes con un buen riesgo crediticio, así como con mal riesgo. Esto lo vemos en el código a continuación:

```{r proporcion riesgo}
credit %>% 
  dplyr::count(risk) %>% 
  dplyr::mutate(prop = n/sum(n))
```

Vemos entonces que el 30% de los clientes tiene un mal riesgo crediticio.

Además, nos interesa ver las posibles relaciones entre las variables predictoras y la variable respuesta. Una opción sería ver cómo distribuyen las edades de las personas al separar por el riesgo, lo cual vemos en el gráfico a continuación.

```{r relacion edad riesgo, echo=FALSE}
ggplot(data = credit,
       mapping = aes(x = risk, y = age, fill = risk)) +
  geom_boxplot() +
  labs(x = 'riesgo', y = 'edad',
       title = 'Relación edad y riesgo del cliente')
```

Notamos que la edad de las personas con un riesgo bajo tiene una mediana mayor que las de riesgo alto, pero que ambos riesgos tienen personas de todas las edades. Les dejo a ustedes ver otras formas de poder analizar de manera exploratoria los datos.

## Modelamiento

Recuerden que con `tidymodels` tenemos pasos bastante marcados:

1. Dividir los datos - `rsample`
2. Especificación del modelo - `parsnip`
3. Pre-procesamiento de los datos y Feature engineering - `recipes`
4. Ajuste del modelo - `workflows`, `parsnip`, `broom`, `tune`
5. Evaluación del modelo - `workflows`, `yardstick`, `tune`, `broom`

**Nota**: 2 y 3 son intercambiables.

Lo anterior queda mejor representado en la siguiente imagen:

<center>![Modelo de regresión logística](https://jhudatascience.org/tidyversecourse/images/book_figures/MachineLearning_tidymodels.png)

</center>


### División de los datos

Realizamos la división en una base de entrenamiento y una de test utilizando la librería `rsample`. Recuerden que tenemos que estratificar por `risk` y así asegurarnos que la proporción de casos se mantenga en ambas particiones.

```{r split data}
split_info <- rsample::initial_split(
  data = credit,
  prop = 0.75,
  strata = risk
)

credit_train <- rsample::training(split_info)
credit_test <- rsample::testing(split_info)
```

Verificamos que efectivamente se mantienen las proporciones.

```{r verificar props}
# Base de entrenamiento
credit_train %>% 
  dplyr::count(risk) %>% 
  dplyr::mutate(prop = n/sum(n))
  
# Base de testeo
credit_test %>% 
  dplyr::count(risk) %>% 
  dplyr::mutate(prop = n/sum(n))
```

### Especificación del modelo

Recuerden que utilizamos la librería `parsnip` para definir el modelo.

```{r especificacion modelo}
# Especificamos el modelo
logreg_model <- 
  parsnip::logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')

# Podemos ver los detalles del modelo
logreg_model %>% 
  parsnip::translate()
```

**Nota**: para `logistic_reg()` el engine por defecto es `glm`, y el modo por defecto es `classification`, por lo que no es necesario usar el `set_engine()` y `set_mode()`. De todas maneras, yo prefiero incluir las funciones ya que personalmente prefiero ser explícito en los códigos.

### Pre-procesamiento de los datos y Feature engineering

Para este paso usamos la librería `recipes`. En nuestro caso nos interesa:

1. Definir la relación entre predictores y variable respuesta
2. Definir los datos a utilizar en el ajuste
3. Definir las transformaciones necesarias a los datos, en particular, queremos centrar y escalar las variables numéricas, así como codificar las variables categóricas.

Lo anterior se encuentra en el siguiente código.

```{r preprocesamiento datos}
# Creamos la receta
credit_recipe <- 
  recipes::recipe(risk ~ ., data = credit_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

# Podemos ver los detalles de la receta
credit_recipe
```

### Ajuste del modelo

Juntamos todo el proceso en un objeto `workflow` y ajustamos el modelo en los datos de entrenamiento, utilizando la librería `parsnip`.

```{r workflow}
# Definimos el workflow (o pipeline)
logreg_wf <-
  workflows::workflow() %>%
  add_model(logreg_model) %>%
  add_recipe(credit_recipe)

# Ajustamos el modelo con parsnip
logreg_fit <- logreg_wf %>%
  parsnip::fit(data = credit_train)

# Podemos ver la receta ajustada
logreg_fit %>%
  extract_recipe(estimated = TRUE)

# Podemos ver el modelo ajustado
logreg_fit %>%
  extract_fit_parsnip() %>%
  broom::tidy()
```

A partir del modelo ajustado obtenemos varios valores importantes, entre los cuales se encuentra el valor estimado del parámetro $\beta_i$, junto al valor-p correspondiente.

Recuerden que el intercepto corresponde al logaritmo de la chance de que el cliente tenga un riesgo crediticio alto, **cuando todos los predictores son 0**. ¿Tiene sentido esto en este caso? hint: recuerden que normalizamos las variables antes de realizar el ajuste del modelo. Así, tenemos que

$$
\log \frac{\theta(0)}{1 - \theta(0)} \approx -0.951 \Rightarrow \theta(0) = \frac{e^{\beta_0}}{1 + e^{\beta_0}} \approx 0.279
$$

Por último, podemos ver las predicciones del modelo en la base de testeo.

```{r prediccion modelo}
# Podemos ver las predicciones del modelo y comparar con el real
credit_test_pred <- credit_test %>% 
  dplyr::select(risk) %>% 
  dplyr::mutate(predict(logreg_fit, new_data = credit_test)) %>% 
  dplyr::rename(risk_pred = .pred_class)

head(credit_test_pred)

## Podemos ver cuantos fueron categorizados como bueno y malo
credit_test_pred %>% 
  dplyr::count(risk_pred) %>% 
  dplyr::mutate(prop = n/sum(n))
```

### Evaluación del modelo

Notemos que en este caso no estamos evaluando diferentes modelos para ver cuál es mejor ni tampoco tenemos que optimizar hiperparámetros, por lo que podemos evaluar directamente el modelo ajustado en la base de test para ver cómo se defiende ante datos nuevos.

En este caso, una forma que tenemos para evaluar el modelo es usando la librería `tune`. Esta librería tiene una función conveniente llamada `last_fit()`, la cual ajusta el modelo con los datos de entrenamiento y realiza la predicción en la base de test, entregando diferentes métricas. Es importante notar que esta función **recibe la información del split**.

```{r evaluacion del modelo}
# last_fit ajusta y predice al mismo tiempo
final_logreg <- 
  logreg_wf %>% 
  tune::last_fit(split_info)

# Obtenemos algunas métricas con collect_metrics()
final_logreg %>% 
  collect_metrics()
```

En particular, `last_fit()` nos entrega dos medidas de evaluación: accuracy y roc_auc, pero de momento no nos preocupemos por la segunda. El **accuracy** nos dice la proporción de los datos en la base de testeo que el modelo fue capaz de predecir correctamente. Vemos que en este caso el modelo es capaz de predecir bien aproximadamente el 70% de las veces.

### Todos los pasos juntos

```{r modelamiento completo}
# 1 - Dividimos los datos
split_info <- rsample::initial_split(
  data = credit,
  prop = 0.75,
  strata = risk
)

credit_train <- rsample::training(split_info)
credit_test <- rsample::testing(split_info)

# 2 - Especificamos el modelo
logreg_model <- 
  parsnip::logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')

# 3 - Creamos la receta
credit_recipe <- 
  recipes::recipe(risk ~ ., data = credit_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

# 4 - Juntamos todo en un workflow 
logreg_wf <-
  workflows::workflow() %>%
  add_model(logreg_model) %>%
  add_recipe(credit_recipe)

# 5 - Ajustamos el modelo
logreg_fit <- logreg_wf %>%
  parsnip::fit(data = credit_train)

# 6 - Evaluamos el modelo
logreg_metrics <- logreg_wf %>% 
  tune::last_fit(split_info) %>% 
  collect_metrics()
```
