---
title: "Ayudantía 9 - ML para Negocios &#127794;&#127794;&#127794;"
date: "9 de noviembre del 2022"
output:
  html_document:
    df_print: paged
    theme: flatly
    highlight: breezedark
    toc: yes
    toc_float: yes
---

```{=html}
<style type="text/css">
/* Whole document: */
@import url('https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible&display=swap');
body{
  font-family: 'Atkinson Hyperlegible', sans-serif;
  font-size: 15pt;
  background-color: #f2f3ed;
	
}
/* Headers */
h1,h2,h3,h4,h5,h6{
  font-size: 20pt;
 font-color:#03DAC6; 
}

div.main-container {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

```{r setup, include=FALSE, message=FALSE}
library(here)


## Global options
knitr::opts_chunk$set(
  cache = TRUE, fig.align = "center",
  fig.height = 7, fig.width = 12
)

# Here
here::i_am("Ayudantía 9/Ayudantía 9.Rmd")
```

# Introducción

Bienvenid\@s a la novena ayudantía de EAA3707 - Machine Learning para Negocios. En la ayudantía veremos:

1.  Métodos de ensamble
    * Bagging
    * Boosting
    * Stacking

Antes de comenzar, cargamos las librerías que utilizaremos durante la ayudantía.

```{r librerias, message=FALSE, warning=FALSE}
library(here)
library(beepr)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(skimr)
library(corrplot)

library(baguette)
library(stacks)
library(rpart)
library(xgboost)


# Para obtener resultados reproducibles
set.seed(219)
```

**Cambiar**: Ocupamos las siguientes librerías nuevas:

*   `beepr`: permite emitir sonidos al terminar de ejecutar un comando. Útil cuando estamos ajustando modelos pesados.
*   `skimr`: permite echar fácilmente un vistazo inicial a los datos.
*   `ranger`: uno de los motores aceptados por `parsnip` para ajustar bosques aleatorios.

Les recuerdo los libros que les recomiendo para aprender de R:

-   **Hands-On Programming with R**: disponible [acá](https://rstudio-education.github.io/hopr/). Útil para recordar cosas básicas de R.

-   **Advanced R**: disponible [acá](https://adv-r.hadley.nz/). Para aprender R avanzado (realmente avanzado), si es que están interesados en entender cómo funciona R por detrás.

-   **R for Data Science**: disponible [acá](https://r4ds.had.co.nz/). Bueno para aprender y aplicar Data Science de manera rápida, además de ser uno de los libros guía para aprender acerca de los paquetes de tidyverse.

-   **RMarkdown Cookbook**: disponible [acá](https://bookdown.org/yihui/rmarkdown-cookbook/). Útil para aprender lo básico de RMarkdown.

-   **RMarkdown: The Definitive Guide**: disponible [acá](https://bookdown.org/yihui/rmarkdown/). Útil para aprender cosas avanzadas de RMarkdown.

-   **Tidy Modeling with R**: disponible [acá](https://www.tmwr.org/). Recomendado para aprender del ecosistema tidymodels. Tiene también un capítulo pequeño de tidyverse con lo justo y necesario.

# Métodos de ensamble

<center>![Métodos de ensamble - Créditos Kristina Grigaitytė, Turing College](https://blog.turingcollege.com/content/images/2021/10/Ensemble-learning.png){height="500px" width="750px"}</center>

Como vieron en clases, los métodos de ensamble ayudan a mejorar los resultados de modelos de Machine Learning, mediante la combinación de varios modelos, generando modelos de predicción robustos. Estos métodos pueden ser utilizados con cualquier modelo, pero generalmente se trabaja con árboles de decisión.

En la ayudantía veremos tres métodos: Bagging, Boosting y Stacking. Los primeros dos son homogéneos, esto es, consideran un conjunto de modelos iguales.

## Bagging

<center>![Bagging (Créditos: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)](https://miro.medium.com/max/4800/1*zAMhmZ78a6V9W878zfk5eA@2x.webp)</center>

El Bagging, acrónimo de Bootstrap Agreggation, es un modelo homogéneo donde cada una de las partes aprende en paralelo de forma independiente, para finalmente combinarlos y predecir la clase más frecuente, en el contexto de clasificación, y el promedio, en el caso de regresión. Esta combinación de predicciones logra que disminuya la varianza.

Para ajustar cada uno de los modelos, bagging selecciona una muestra bootstrap (muestreo con reemplazo) de la base original.

**Nota**: Los Bosques Aleatorios son un caso especial de Bagging.

## Boosting

<center>![Boosting (Créditos: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)](https://miro.medium.com/max/4800/1*VGSoqefx3Rz5Pws6qpLwOQ@2x.webp)</center>

El Boosting también es un modelo homogéneo, pero que funciona de forma secuencial y adaptativa, esto es, cada modelo adicional busca corregir los errores del modelo anterior.

En particular, existen dos métodos de boosting: Gradient boosting y Adaptive boosting (AdaBoost). Nosotros nos enfocaremos en Gradient Boosting.

## Stacking

<center>![Stacking (Créditos: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)](https://miro.medium.com/max/1400/1*ZucZsXkOwrpY2XaPh6teRw@2x.webp)</center>

Este método se diferencia de los dos anteriores en que este (casi siempre) considera modelos heterogéneos. Así, en este caso, podemos mezclar modelos diferentes, como por ejemplo un SVM, un KNN y un árbol de decisión.

Para combinar los resultados, Stacking considera un meta-modelo, al cual se le entregan los resultados de los modelos débiles anteriores.

## ¿Cuándo utilizar cada uno?

Dado que los modelos básicos de Bagging se ajustan en paralelo, esta técnica suele utilizarse con modelos de bajo sesgo pero alta variabilidad. Esto es debido a que los modelos de estas características suelen ser mas costosas computacionalmente, así nos aprovechamos de la paralelización.

Por otro lado, como Boosting ajusta modelos uno por uno, esta técnica se suele utilizar para modelos de alto sesgo pero baja variabilidad.


# Ejemplo: Predición de depósitos a largo plazo

<center>![Créditos: Bank for Investment and Development of Vietnam (BIDV)](https://static.tapchitaichinh.vn/images/upload/hoangthuviet/01182022/chinh-sach-tai-khoa.jpg){height="500px" width="900px"}</center>

https://www.kaggle.com/datasets/rashmiranu/banking-dataset-classification?select=new_train.csv


Harto del número reducido de empresas de teléfonos celulares, usted decide iniciar su propia compañía que logre competir con las diferentes empresas globales importantes.

Uno de los problemas es poder estimar el precio de los celulares que producirá, para lo cual decide recolectar datos de diferentes celulares y el rango de precios al que pertenecen. Utilizando estos datos, desea entrenar un modelo de clasificación que le permita definir un rango de precios para sus futuros productos.

Estos datos se encuentran en la base de datos `mobile_price.csv`. Entre las variables se encuentran:

* **battery_power**: energía total que puede almacenar, en mAh
* **blue**: tiene bluetooth o no
* **clock_speed**: velocidad del microproceasador
* **dual_sim**: tiene soporte para dual sim o no
* **fc**: megapixeles de la cámara frontal (0 si no tiene cámara frontal)
* **four_g**: si tiene o no 4G
* **int_memory**: memoria interna en GB
* **pc**: megapixeles de la cámara principal
* **px_height**: resolución a lo alto
* **px_widht**: resolución a lo ancho
* **ram**: memoria RAM del dispositivo en megabytes
* **sc_h**: alto de la pantalla en centímetros
* **sc_w**: ancho de la pantalla en centímetros
* **price_range**: variable respuesta. Toma los valores 0 (bajo costo), 1 (costo medio), 2 (costo alto) y 3 (costo muy alto)

## Análisis exploratorio

Importemos en primer lugar nuestros datos y demos un primer vistazo inicial.

```{r carga datos}
# Cargamos los datos
mobile_prices_unclean <- readr::read_csv(here("Ayudantía 8", "mobile_price.csv"),
                                         show_col_types = FALSE)

# Damos un vistazo
skimr::skim(mobile_prices_unclean)
```

Notamos en particular que algunas variables que deberían ser factor aparecen como numéricas, así que debemos cambiarlas.

```{r cambio a factor}
mobile_prices <- mobile_prices_unclean %>% 
  dplyr::mutate(blue = factor(blue, levels=c(1, 0), labels=c("Yes", "No")),
                dual_sim = factor(dual_sim, levels=c(1, 0), labels=c("Yes", "No")),
                four_g = factor(four_g, levels=c(1, 0), labels=c("Yes", "No")),
                three_g = factor(three_g, levels=c(1, 0), labels=c("Yes", "No")),
                touch_screen = factor(touch_screen, levels=c(1, 0),
                                      labels=c("Yes", "No")),
                wifi = factor(wifi, levels=c(1, 0), labels=c("Yes", "No")),
                price_range = factor(price_range, levels=c(0, 1, 2, 3),
                                     labels=c("Low", "Medium", "High", "Very High"),
                                     ordered = TRUE))

skimr::skim(mobile_prices)
```

Así, tenemos 14 variables numéricas y 7 categóricas, incluyendo la variable respuesta.

Como análisis exploratorio, podemos ver en primer lugar la correlación entre las variables númericas presentes en la base de datos.

```{r correlacion, echo=FALSE}
mobile_prices %>% 
  select_if(is.numeric) %>% 
  cor() %>% 
  corrplot::corrplot()
```

Notamos que existe una baja correlación excepto entre variables que intuitivamente sí deberían estar correlacionadas entre sí.

Por otro lado, podemos elegir dos variables al azar y realizar boxplots para ver si existen relaciones entre alguna de ellas con el rango de precios. Para este ejemplo, tomé la velocidad del procesador y la memoria RAM.

```{r boxplots, echo=FALSE}
p1 <- ggplot(mobile_prices,
             mapping = aes(x = price_range, y = clock_speed, fill = price_range)) +
  geom_boxplot() +
  labs(title = "Relación entre velocidad del procesador y rango de precio",
       x = "Rango de precio", y = "Velocidad del procesador")

p2 <- ggplot(mobile_prices,
             mapping = aes(x = price_range, y = ram, fill = price_range)) +
  geom_boxplot() +
  labs(title = "Relación entre memoria RAM y rango de precio",
       x = "Rango de precio", y = "Memoria RAM")

p1 + p2
```

Notamos que pareciera no existir una relación con la primera variable, pero sí una clara relación positiva entre la memoria RAM con el precio.

