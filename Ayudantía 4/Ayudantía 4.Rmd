---
title: "Ayudantía 4 - ML para Negocios"
date: "7 de septiembre del 2022"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "light"
    downcute_theme: "default"
---

```{=html}
<style type="text/css">
body{
  font-size: 15pt;
}
.Wrap {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

```{r setup, include=FALSE, message=FALSE}
library(here)


## Global options
knitr::opts_chunk$set(cache = TRUE)

# Here
here::i_am('Ayudantía 4/Ayudantía 4.Rmd')
```

# Introducción

Bienvenid\@s a la cuarta ayudantía de EAA3707 - Machine Learning para Negocios. En la ayudantía veremos:

1.  Recordatorio modelo de regresión logística
2.  Supuestos del modelo de regresión logística
3.  Algunos tests de hipótesis
3.  Discusión: Regresión logística altamente no balanceada

Antes de comenzar, cargamos las librerías que utilizaremos durante la ayudantía.

```{r librerias, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(car)


# Para obtener resultados reproducibles
set.seed(912)
```

La librería `car` será utilizada para realizar diferentes tests para evaluar los supuestos del modelo de regresión logística.

Además, les recuerdo los libros que les recomiendo para aprender de R:

-   **Hands-On Programming with R**: disponible [acá](https://rstudio-education.github.io/hopr/). Útil para recordar cosas básicas de R.

-   **Advanced R**: disponible [acá](https://adv-r.hadley.nz/). Para aprender R avanzado (realmente avanzado), si es que están interesados en entender cómo funciona R por detrás.

-   **R for Data Science**: disponible [acá](https://r4ds.had.co.nz/). Bueno para aprender y aplicar Data Science de manera rápida, además de ser uno de los libros guía para aprender acerca de los paquetes de tidyverse.

-   **RMarkdown Cookbook**: disponible [acá](https://bookdown.org/yihui/rmarkdown-cookbook/). Útil para aprender lo básico de RMarkdown.

-   **RMarkdown: The Definitive Guide**: disponible [acá](https://bookdown.org/yihui/rmarkdown/). Útil para aprender cosas avanzadas de RMarkdown.

-   **Tidy Modeling with R**: disponible [acá](https://www.tmwr.org/). Recomendado para aprender del ecosistema tidymodels. Tiene también un capítulo pequeño de tidyverse con lo justo y necesario.

# Recuerdo del modelo de regresión logística

<center>![Modelo de regresión logística](https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning.png)</center>

Antes de partir con la ayudantía, es importante recordar los conceptos y definiciones importantes del modelo de regresión logística.

Recordemos que en el caso de regresión logística el modelo puede ser escrito como:

$$
\begin{align}
y_i|\mathbf{x}_i &\overset{ind}{\sim} \text{Bernoulli}(\theta_i) \\
g(\theta_i) &= \beta_0 + \beta_1x_{i, 1} + ... + \beta_p x_{i, p} = x^T \beta
\end{align}
$$

donde $g$ es la función logit, esto es,

$$
g(\theta_i) = \text{logit }\theta_i = \log \frac{\theta_i}{1 - \theta_i}
$$

En palabras, lo anterior nos dice que proponemos un modelo en el cual se modela la variable respuesta, $Y$, como realizaciones de una variable aleatoria Bernoulli, esto es, $Y=1$ con probabilidad $\theta$ o $Y=0$ con probabilidad $1 - \theta$, donde el **logit de la media** de la distribución se modela como una función lineal de ciertos predictores o covariables.

--
**Nota**: recuerde que si $Y \sim \text{Bernoulli}(p)$ entonces $E(Y) = p$, por lo que el parámetro corresponde justamente a la media. Esto no necesariamente sucede con cualquier modelo.
--

De manera equivalente, notemos que

$$
g(\theta_i) = \log \frac{\theta_i}{1 - \theta_i} = x_i^T \beta\Leftrightarrow \theta_i = \frac{\exp(x^T\beta)}{1 + \exp(x^T\beta)}
$$

por lo tanto, también podemos decir que la **media** de la distribución es la función logística de la componente lineal $x^T\beta$ (a veces denominada componente sistemática)

--
**Nota**: Como se mencionó la semana pasada, lo único que necesitamos de la función $g$ es que lleve valores de $(0, 1)$ a $(-\infty, \infty)$. La función logit es solo una de ellas, pero está también por ejemplo la función probit o la log-log-complementaria (de hecho, si se fijan la inversa de cualquier función de distribución acumulada cumple lo pedido). Pese a esto, en la práctica el modelo se sigue denominando regresión logística, solo que decimos que cambiamos la **función de enlace**.
--

En la figura a continuación podemos ver las diferencias entre los enlaces logit, probit y log-log-complementario.

```{r distintos enlaces, echo=FALSE, fig.align='center'}
logistic <- function(x){exp(x)/(exp(x) + 1)}
probit <- function(x){pnorm(x)}
llc <- function(x){1 - exp(-exp(x))}

ggplot(data.frame(x = c(-10, 10)), aes(x = x)) + 
  stat_function(fun = logistic, col = 'red', lwd = 1) +
  stat_function(fun = probit, col = 'blue', lwd = 1) +
  stat_function(fun = llc, col = 'green', lwd = 1) +
  geom_hline(yintercept = c(0, 1), lty = 'dashed') +
  geom_hline(yintercept = 0.5, lty = 'dashed') +
  geom_vline(xintercept = 0, lty = 'dashed') +
  ylim(-0.1, 1.1) +
    labs(title = 'Diferentes funciones de enlace',
         subtitle = 'Rojo - logit, Azul - probit, Verde - llc',
         y = 'p(Y=1|x)')
```

Notamos de lo anterior que tanto el logit como el probit son funciones de enlace simétricas, pero no así el enlace log-log-complementario.

## Definiciones importantes

También es importante recordar la siguiente terminología, que es muy utilizada en el contexto de regresión logística.

Las **chances** u **odds** de un suceso es el cociente entre sus probabilidades de ocurrencia y sus probabilidades de no ocurrencia, esto es,

$$
\text{Odds}(A) = \frac{P(A)}{1 - P(A)}
$$

En el caso de regresión logística con $p$ predictores, el odds de un suceso es el cociente entre sus probabilidades de ocurrencia y sus probabilidades de no ocurrencia, dejando $x$ fijo, esto es

$$
\text{Odds}(Y=1|x) = \frac{\theta(x)}{1 - \theta(x)} = \exp(x^T\beta)
$$

El cambio proporcional en los odds se calcula dividiendo los odds después de un cambio y los odds antes del cambio, lo que se conoce como la **razón de chances** u **odds ratio**.

En el caso de regresión logística, tenemos que si el predictor $j$ aumenta una unidad, entonces

$$
\text{OR}(x_j + 1, x_j) = \frac{\text{Odds}(Y=1|x_j + 1)}{\text{Odds}(Y = 1|x_j)} = \exp(\beta_j)
$$

luego, $\exp(\beta_j)$ se interpreta como el factor por el que cambian las chances cuando el predictor $j$, $x_j$, aumenta en una unidad.

# Supuestos del modelo de regresión logística

Cada modelo que proponemos para describir la realidad se basan, en mayor o menor medida, supuestos que hacemos para simplificar el análisis, sin una gran pérdida de generalidad.

En particular, el modelo de regresión logística también se basa en ciertos supuestos, entre los cuales se encuentran:

1. Linealidad del logit: recordemos que el modelo plantea que
$$
\log \frac{\theta_i}{1 - \theta_i} = x_i^T \beta
$$
2. Independencia de las observaciones
3. No-colinealidad de los predictores

Es importante notar que algunos de estos supuestos pueden ser estudiados de manera visual, antes de ajustar los datos, o de manera más formal, lo cual usualmente se realiza **después** de ajustar el modelo. Es importante notar que los chequeos visuales a veces no sirven mucho, ya que no podemos ver en más de 3 dimensiones.

¿Por qué después? porque aprovechamos las propiedades distribucionales de los datos, con lo cual podemos definir estadísticos muestrales. En particular, en los modelos de regresión es común utilizar los **residuos** $y_i - \hat{y}_i$ y aprovecharnos de algunas propiedades distribucionales asintóticas.

**Nota**: podrán notar que el supuesto de homocedasticidad no está presente en el caso de regresión logística.

## Ejemplo práctico

Para estudiar los supuestos anteriores, utilizaremos la misma base de datos de la ayudantía pasada, `credit.data`. Recordemos que esta base contiene información crediticia de diferentes clientes de un banco alemán. En particular, se tiene información de 1000 clientes, con sus diferentes atributos, así como una variable que indica si la persona tiene un mal riesgo crediticio.

**Nota**: recuerden que pueden acceder a toda la información de los datos en el siguiente [enlace](https://archive-beta.ics.uci.edu/ml/datasets/statlog+german+credit+data)

Carguemos entonces la base de datos en el código a continuación. Además, realizamos el preprocesamiento de eliminar algunas variables, cambiar la codificación y agregar nombres informativos a las variables.

```{r carga datos, message=FALSE}
# Cargamos los datos
credit_full <- readr::read_table(
  file = here::here('Ayudantía 4', 'german.data'), 
  col_names = FALSE
)

# Pre-procesamiento de los datos
credit <- 
  credit_full %>% 
  dplyr::select(X2, X3, X5, X13, X15, X16, X21) %>% 
  dplyr::rename(duration = X2,
                credit_history = X3,
                credit_amount = X5,
                age = X13,
                housing = X15,
                n_credits = X16,
                risk = X21) %>%
  dplyr::mutate(risk = risk - 1) %>% # Parte nueva
  dplyr::mutate(housing = dplyr::recode(housing, 
                                        "A151" = 'rent',
                                        "A152" = 'own',
                                        "A153" = 'free'),
                risk = dplyr::recode(risk,
                                     `0` = 'good',
                                     `1` = 'bad')) %>% 
  dplyr::mutate(across(where(is.character), as_factor))

# Vemos cómo nos quedan
dplyr::glimpse(credit)
```

Ya teniendo nuestros datos listos, pasamos a ver el primer supuesto, que es el de la linealidad del logit.

## Linealidad del logit (visual)

Con un predictor $x$ podemos graficar las proporciones muestrales $\sum y_i/n_i$ versus $x_i$, donde $n_i$ es el número de observaciones con valor del predictor $x_i$, lo cual debiese tener forma de S. Equivalentemente, podemos graficar los logitos muestrales versus $x_i$ lo cual debería ser aproximadamente lineal. Pero, ¿cuáles son los problemas con este enfoque?

Primero, si $x$ es continuo, podemos tener que $n_i = 1$ para muchos valores, por lo cual nisiquiera podríamos calcular el logito (obtendríamos $-\infty$ o $\infty$). Una solución sería una pequeña corrección para evitar estos valores problemáticos. Además, si siempre se tiene $n_i = 1$ entonces $y_i/n_i$ tendría solo dos valores posibles, por lo que no podríamos visualizar bien.

Otro problema importante es que esta técnica funcionaría solo en el caso de un predictor (o a lo más dos si usamos gráficos 3D), pero no para los casos generales que son más comunes. Además, puede ser que si vemos gráficos 2D por separado pensemos que la relación no es lineal, pero al verlo de manera conjunta sí existe esta relación.

De todas maneras, veamos dos ejemplo visuales. Primero utilizaremos la variable `Age` que no tiene los problemas de muchos $n_i = 1$:

```{r ejemplo Age, fig.align='center'}
# Podemos contar según la edad, vemos que existe un n_i = 1
age_freqs <- credit %>% 
  dplyr::count(age, name = 'total')
age_freqs
any(age_freqs$total == 1)
sum(age_freqs$total == 1)

# Agrupamos por edad y calculamos las proporciones y logitos muestrales
prop_logit <- credit %>% 
  dplyr::group_by(age, risk, .drop = FALSE) %>% 
  dplyr::count() %>% 
  dplyr::filter(risk == 'bad') %>% 
  dplyr::left_join(age_freqs) %>% 
  dplyr::mutate(prop = n/total)

logitos <- car::logit(prop_logit$prop, adjust = 0.01)
prop_logit$logitos = logitos

# Graficamos
## Proporciones
ggplot(prop_logit, mapping = aes(x = age, y = prop)) +
  geom_point() +
  labs(title = 'Análisis visual proporciones muestrales',
       subtitle = 'Usando la variable Age',
       x = 'Edad', y = 'Proporción muestral')

## Logitos
ggplot(prop_logit, mapping = aes(x = age, y = logitos)) +
  geom_point() +
  labs(title = 'Análisis visual linealidad del logito',
       subtitle = 'Usando la variable Age',
       x = 'Edad', y = 'Proporción muestral')
```

A través del gráfico podemos ver que al parecer no se cumple el supuesto de linealidad del logito **si sólo consideramos la variable Age**.

Ahora veamos una variable continua, en particular la variable `credit_amount`. En el caso continuo, si no se repiten mucho los valores, es conveniente agrupar los datos en categorías.

```{r ejemplo credit amount, fig.align='center'}
# Podemos contar según la edad, vemos que en este caso hay 847 n_i = 1
am_freqs <- credit %>% 
  dplyr::count(credit_amount, name = 'total')
am_freqs
sum(am_freqs$total == 1)

# Agrupamos las edades en intervalos
binned <- credit %>% 
  dplyr::mutate(amount_bin = cut(credit_amount, breaks = 10, labels = FALSE)) %>% 
  dplyr::select(credit_amount, amount_bin, risk)

amb_freqs <- binned %>% 
  dplyr::count(amount_bin, name = 'total')

amb_freqs

# Realizamos el conteo y calculamos proporciones y logitos
prop_logit <- binned %>% 
  dplyr::group_by(amount_bin, risk, .drop = FALSE) %>% 
  dplyr::count() %>% 
  dplyr::filter(risk == 'bad') %>% 
  dplyr::left_join(amb_freqs) %>% 
  dplyr::mutate(prop = n/total)

logitos <- car::logit(prop_logit$prop, adjust = 0.01)
prop_logit$logitos = logitos

# Graficamos
## Proporciones
ggplot(prop_logit, mapping = aes(x = amount_bin, y = prop)) +
  geom_point() +
  labs(title = 'Análisis visual proporciones muestrales',
       subtitle = 'Usando la variable Credit Amount',
       x = 'Monto del crédito ajustado', y = 'Proporción muestral')

## Logitos
ggplot(prop_logit, mapping = aes(x = amount_bin, y = logitos)) +
  geom_point() +
  labs(title = 'Análisis visual linealidad del logito',
       subtitle = 'Usando la variable Credit Amount',
       x = 'Monto del crédito ajustado', y = 'Proporción muestral')
```

Vemos que en este caso sí pareciera existir una pequeña relación lineal **sólo si consideramos el monto del crédito**.

## Modelamiento

Recuerden que con `tidymodels` tenemos pasos bastante marcados:

1. Dividir los datos - `rsample`
2. Especificación del modelo - `parsnip`
3. Pre-procesamiento de los datos y Feature engineering - `recipes`
4. Ajuste del modelo - `workflows`, `parsnip`, `broom`, `tune`
5. Evaluación del modelo - `workflows`, `yardstick`, `tune`, `broom`

**Nota**: 2 y 3 son intercambiables.

Lo anterior queda mejor representado en la siguiente imagen:

<center>![Modelo de regresión logística](https://jhudatascience.org/tidyversecourse/images/book_figures/MachineLearning_tidymodels.png)

</center>


### División de los datos

Realizamos la división en una base de entrenamiento y una de test utilizando la librería `rsample`. Recuerden que tenemos que estratificar por `risk` y así asegurarnos que la proporción de casos se mantenga en ambas particiones.

```{r split data}
split_info <- rsample::initial_split(
  data = credit,
  prop = 0.75,
  strata = risk
)

credit_train <- rsample::training(split_info)
credit_test <- rsample::testing(split_info)
```

Verificamos que efectivamente se mantienen las proporciones.

```{r verificar props}
# Base de entrenamiento
credit_train %>% 
  dplyr::count(risk) %>% 
  dplyr::mutate(prop = n/sum(n))
  
# Base de testeo
credit_test %>% 
  dplyr::count(risk) %>% 
  dplyr::mutate(prop = n/sum(n))
```

### Especificación del modelo

Recuerden que utilizamos la librería `parsnip` para definir el modelo.

```{r especificacion modelo}
# Especificamos el modelo
logreg_model <- 
  parsnip::logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')

# Podemos ver los detalles del modelo
logreg_model %>% 
  parsnip::translate()
```

**Nota**: para `logistic_reg()` el engine por defecto es `glm`, y el modo por defecto es `classification`, por lo que no es necesario usar el `set_engine()` y `set_mode()`. De todas maneras, yo prefiero incluir las funciones ya que personalmente prefiero ser explícito en los códigos.

### Pre-procesamiento de los datos y Feature engineering

Para este paso usamos la librería `recipes`. En nuestro caso nos interesa:

1. Definir la relación entre predictores y variable respuesta
2. Definir los datos a utilizar en el ajuste
3. Definir las transformaciones necesarias a los datos, en particular, queremos centrar y escalar las variables numéricas, así como codificar las variables categóricas.

Lo anterior se encuentra en el siguiente código.

```{r preprocesamiento datos}
# Creamos la receta
credit_recipe <- 
  recipes::recipe(risk ~ ., data = credit_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

# Podemos ver los detalles de la receta
credit_recipe
```

### Ajuste del modelo

Juntamos todo el proceso en un objeto `workflow` y ajustamos el modelo en los datos de entrenamiento, utilizando la librería `parsnip`.

```{r workflow}
# Definimos el workflow (o pipeline)
logreg_wf <-
  workflows::workflow() %>%
  add_model(logreg_model) %>%
  add_recipe(credit_recipe)

# Ajustamos el modelo con parsnip
logreg_fit <- logreg_wf %>%
  parsnip::fit(data = credit_train)

# Podemos ver la receta ajustada
logreg_fit %>%
  extract_recipe(estimated = TRUE)

# Podemos ver el modelo ajustado
logreg_fit %>%
  extract_fit_parsnip() %>%
  broom::tidy()
```

A partir del modelo ajustado obtenemos varios valores importantes, entre los cuales se encuentra el valor estimado del parámetro $\beta_i$, junto al valor-p correspondiente.

Recuerden que el intercepto corresponde al logaritmo de la chance de que el cliente tenga un riesgo crediticio alto, **cuando todos los predictores son 0**. ¿Tiene sentido esto en este caso? hint: recuerden que normalizamos las variables antes de realizar el ajuste del modelo. Así, tenemos que

$$
\log \frac{\theta(0)}{1 - \theta(0)} \approx -0.951 \Rightarrow \theta(0) = \frac{e^{\beta_0}}{1 + e^{\beta_0}} \approx 0.279
$$

Por último, podemos ver las predicciones del modelo en la base de testeo.

```{r prediccion modelo}
# Podemos ver las predicciones del modelo y comparar con el real
credit_test_pred <- credit_test %>% 
  dplyr::select(risk) %>% 
  dplyr::mutate(predict(logreg_fit, new_data = credit_test)) %>% 
  dplyr::rename(risk_pred = .pred_class)

head(credit_test_pred)

## Podemos ver cuantos fueron categorizados como bueno y malo
credit_test_pred %>% 
  dplyr::count(risk_pred) %>% 
  dplyr::mutate(prop = n/sum(n))
```

### Evaluación del modelo

Notemos que en este caso no estamos evaluando diferentes modelos para ver cuál es mejor ni tampoco tenemos que optimizar hiperparámetros, por lo que podemos evaluar directamente el modelo ajustado en la base de test para ver cómo se defiende ante datos nuevos.

En este caso, una forma que tenemos para evaluar el modelo es usando la librería `tune`. Esta librería tiene una función conveniente llamada `last_fit()`, la cual ajusta el modelo con los datos de entrenamiento y realiza la predicción en la base de test, entregando diferentes métricas. Es importante notar que esta función **recibe la información del split**.

```{r evaluacion del modelo}
# last_fit ajusta y predice al mismo tiempo
final_logreg <- 
  logreg_wf %>% 
  tune::last_fit(split_info)

# Obtenemos algunas métricas con collect_metrics()
final_logreg %>% 
  collect_metrics()
```

En particular, `last_fit()` nos entrega dos medidas de evaluación: accuracy y roc_auc, pero de momento no nos preocupemos por la segunda. El **accuracy** nos dice la proporción de los datos en la base de testeo que el modelo fue capaz de predecir correctamente. Vemos que en este caso el modelo es capaz de predecir bien aproximadamente el 70% de las veces.

### Todos los pasos juntos

```{r modelamiento completo}
# 1 - Dividimos los datos
split_info <- rsample::initial_split(
  data = credit,
  prop = 0.75,
  strata = risk
)

credit_train <- rsample::training(split_info)
credit_test <- rsample::testing(split_info)

# 2 - Especificamos el modelo
logreg_model <- 
  parsnip::logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')

# 3 - Creamos la receta
credit_recipe <- 
  recipes::recipe(risk ~ ., data = credit_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

# 4 - Juntamos todo en un workflow 
logreg_wf <-
  workflows::workflow() %>%
  add_model(logreg_model) %>%
  add_recipe(credit_recipe)

# 5 - Ajustamos el modelo
logreg_fit <- logreg_wf %>%
  parsnip::fit(data = credit_train)

# 6 - Evaluamos el modelo
logreg_metrics <- logreg_wf %>% 
  tune::last_fit(split_info) %>% 
  collect_metrics()
```
