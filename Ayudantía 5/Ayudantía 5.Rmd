---
title: "Ayudantía 5 - ML para Negocios"
date: "21 de septiembre del 2022"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "dark"
    downcute_theme: "default"
---

```{=html}
<style type="text/css">
body{
  font-size: 15pt;
}
.Wrap {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

```{r setup, include=FALSE, message=FALSE}
library(here)


## Global options
knitr::opts_chunk$set(cache = TRUE, fig.align = 'center',
                      fig.height = 6, fig.width = 10)

# Here
here::i_am('Ayudantía 5/Ayudantía 5.Rmd')
```

# Introducción

Bienvenid\@s a la quinta ayudantía de EAA3707 - Machine Learning para Negocios. En la ayudantía veremos:

1.  K-Nearest Neighbors
2.  Aplicación en regresión
3.  Elección del número de vecinos
4.  Discusión: Curse of dimensionality

Antes de comenzar, cargamos las librerías que utilizaremos durante la ayudantía.

```{r librerias, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(kknn)
library(udunits2)


# Para obtener resultados reproducibles
set.seed(2211)
```

La librería `udunits2` nos permitirá hacer cambios de unidades imperiales a métricas. La librería `kknn` nos permite ajustar modelos de k-vecinos cercanos y es la utilizada por tidymodels como motor.

Les recuerdo los libros que les recomiendo para aprender de R:

-   **Hands-On Programming with R**: disponible [acá](https://rstudio-education.github.io/hopr/). Útil para recordar cosas básicas de R.

-   **Advanced R**: disponible [acá](https://adv-r.hadley.nz/). Para aprender R avanzado (realmente avanzado), si es que están interesados en entender cómo funciona R por detrás.

-   **R for Data Science**: disponible [acá](https://r4ds.had.co.nz/). Bueno para aprender y aplicar Data Science de manera rápida, además de ser uno de los libros guía para aprender acerca de los paquetes de tidyverse.

-   **RMarkdown Cookbook**: disponible [acá](https://bookdown.org/yihui/rmarkdown-cookbook/). Útil para aprender lo básico de RMarkdown.

-   **RMarkdown: The Definitive Guide**: disponible [acá](https://bookdown.org/yihui/rmarkdown/). Útil para aprender cosas avanzadas de RMarkdown.

-   **Tidy Modeling with R**: disponible [acá](https://www.tmwr.org/). Recomendado para aprender del ecosistema tidymodels. Tiene también un capítulo pequeño de tidyverse con lo justo y necesario.

# Algoritmo K-vecinos cercanos (KNN)

<center>![Algoritmo K-vecinos cercanos](https://www.jcchouinard.com/wp-content/uploads/2021/08/image-8.png){height="500px" width="750px"}</center>

El algoritmo K-vecinos cercanos (KNN) es uno de los algoritmos de aprendizaje supervisado más simples e intuitivos, y el cual puede ser utilizado tanto en el contexto de regresión como en el de clasificación.

Este algoritmo realizará la predicción de acuerdo a un número $k$ de vecinos que se encuentren más cercanos a la observación. En particular, en el contexto de clasificación, tendremos que la observación se asignará a la clase con un mayor número de instancias dentro de estos vecinos, mientras que en el caso de regresión se toma un promedio de las respuestas.

Con lo anterior podemos notar que este método no realiza ningún ajuste por detrás, como lo hace por ejemplo una regresión lineal o logística para obtener estimaciones de los parámetros que definen aquellos modelos.

Acá es **MUY** importante tener siempre en cuenta que este método trabaja con el concepto de distancia entre observaciones. En nuestro caso trabajaremos con distancia euclideana, pero si investigan notaran que existen otras distancias, las que también pueden depender del tipo de dato con el que trabajen (por ejemplo distancias entre palabras)

Considerando lo anterior, si nuestros predictores están en escalas diferentes entonces será necesario estandarizar nuestros datos. Por ejemplo, si queremos comparar casas entre sí y tenemos tanto el precio en pesos como el tamaño en metros cuadrados, entonces las diferencias de precios, que estarán por los millones, dominará completamente la distancia entre observaciones.

# Aplicación - Transacciones de bienes raíces

<center>![](https://www.vmcdn.ca/f/files/kamloopsmatters/images/stock-photos/housing/home-for-sale.jpg;w=960){height="500px" width="900px"}</center>

En clases ya vieron el uso del algoritmo KNN para el caso que la variable respuesta es categórica, por lo que acá consideraremos el otro caso correspondiente a una variable respuesta numérica.

En particular, si denotamos por $N_k(x)$ el conjunto de las k observaciones más cercanas a x, entonces tenemos que el estimador KNN está dado por:

$$
\hat{Y}(x) = \frac{1}{k}\sum_{x_i \in N_k(x)} y_i
$$

Como una aplicación práctica, utilizaremos la base de datos `Sacramento` la cual contiene información de 932 transacciones de bienes raíces en el condado de Sacramento, California. En particular, nos interesa poder predecir el precio al que se venderá una casa utilizando diferentes atributos como el tamaño de la casa, el número de piezas, el número de baños, etc.

**Nota**: estos datos vienen en la librería `modeldata`, la cual se carga automáticamente al cargar la librería `tidymodels`. Para más información de la base de datos, pueden utilizar `?Sacramento`. Es importante notar que el tamaño de las casas viene en unidades imperiales (pies), así que los paso a unidades métricas (metros).

```{r cambio unidades}
Sacramento$sqm <- udunits2::ud.convert(Sacramento$sqft, 'ft', 'm')
```

Para simplificar el análisis y poder visualizar de mejor manera los resultados utilizaremos solo la variable `sqm` para poder predecir el precio. La extensión del algoritmo al caso multivariado es directo, aunque hay que tener cuidado con respecto a las diferencias de magnitudes entre variables, así como el tipo de variable.

## Análisis exploratorio

Dado que nos interesa predecir el precio, veamos en primer lugar cómo distribuye esta variable.

```{r ae - precio, echo=FALSE}
ggplot(Sacramento, aes(x = price)) +
  geom_histogram(aes(y = ..density..), bins = 30, col = 'black', fill = 'salmon') +
  geom_density(col = 'blue', lwd = 2) +
  scale_x_continuous(labels = dollar_format()) +
  theme(text = element_text(size = 12)) +
  labs(title = 'Distribución del precio de las casas',
       subtitle = 'Condado de Sacramento, California',
       x = 'Precio (Dólares)', y = 'Densidad')
```

Por otro lado, considerando que nos interesa poder predecir el precio de venta a partir del tamaño en metros cuadrados de la casa, graficamos ambas variables en la misma figura.

```{r ae - relacion sqm y precio, echo=FALSE}
ggplot(Sacramento, aes(x = sqm, y = price)) +
  geom_point() +
  theme(text = element_text(size = 12)) + 
  scale_y_continuous(labels = dollar_format()) +
  labs(title = 'Relación tamaño y precio',
       subtitle = 'Condado de Sacramento, California',
       x = 'Tamaño (Metros cuadrados)', y = 'Precio (Dólares)')
```

Notamos que efectivamente a medida que el tamaño de la casa aumenta entonces el precio también tiende a aumentar.

Por último, si nos interesaría utilizar la variable que nos indica el número de piezas, podemos ver cómo distribuyen los precios para cada uno de los valores presentes.

```{r ae - relacion piezas y precio, echo=FALSE, warning=FALSE}
ggplot(Sacramento, aes(x = as.factor(beds), y = price, fill = as.factor(beds))) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  theme(text = element_text(size = 12)) +
  scale_y_continuous(labels = dollar_format()) +
  labs(title = 'Relación número de piezas y precio de la casa',
       subtitle = 'Condado de Sacramento, California',
       x = 'Número de habitaciones', y = 'Precio (Dólares)',
       fill = 'Número de habitaciones')
```

## Ejemplo visual KNN

Antes de implementar KNN utilizando tidymodels, veamos primero un ejemplo visual acerca de cómo funciona este algoritmo en el caso de regresión. Para esto, tomaremos una muestra pequeña de la base original con la cual realizaremos la predicción para una nueva observación.

Supongamos que estamos interesados en comprar una casa, la cual tiene un tamaño de 550 metros cuadrados, y se encuentra listada a un precio de 250mil dólares. Nos interesa entonces ver si este precio es adecuado o no, utilizando la información que tenemos de las casas con características similares.

```{r ejemplo knn - muestra, fig.height=6, fig.width=10}
# Tomamos una muestra pequeña
small_sacramento <- dplyr::slice_sample(Sacramento, n = 30)

# Vemos la muestra y visualizamos lo que queremos predecir
small_plot <- ggplot(small_sacramento, aes(x = sqm, y = price)) +
  geom_point() +
  geom_vline(xintercept = 550, lty = 'dashed', col = 'red') +
  scale_y_continuous(labels = dollar_format()) +
  theme(text = element_text(size = 12)) +
  labs(title = 'Pequeña muestra de los datos junto a la nueva predicción',
       x = 'Tamaño (Metros cuadrados)', y = 'Precio (Dólares)')

small_plot
```

Para realizar esta predicción, consideraremos usar los 7 vecinos más cercanos con respecto al tamaño, lo cual podemos visualizar a continuación.

```{r ejemplo knn - vecinos}
# Obtenemos los vecinos
vecinos <- small_sacramento %>% 
  dplyr::mutate(diff = abs(550 - sqm)) %>% 
  dplyr::arrange(diff) %>% 
  slice(1:7) %>% 
  dplyr::mutate(xend = rep(550, 7))

# Vemos los vecinos
knitr::kable(vecinos)

# Graficamos las distancias
vecinos_plot <- small_plot +
  geom_segment(data = vecinos,
               aes(x = sqm, xend = xend, y = price, yend = price),
               col = 'blue')
vecinos_plot
```

Por último, utilizamos los precios de estos 7 vecinos para obtener una predicción. Una de las formas que podemos utilizar es a través de un promedio simple, pero también podríamos utilizar promedios ponderados, donde el ponderador es de acuerdo a la distancia.

```{r ejemplo knn - predicción}
# Realizamos la predicción
prediccion <- mean(vecinos$price)

# Visualizamos
vecinos_plot +
  geom_point(aes(x = 550, y = prediccion), col = 'purple', size = 3)
```

Obtenemos entonces un valor de $235.357, por lo que podríamos pensar que la casa se encuentra sobrevalorada.

Este análisis puede ser mejorado en tres sentidos:

1. Como se comentó antes, es un poco más intuitivo que los precios de las casas más cercanas a el punto de interés tengan un mayor peso en el promedio.  

2. Podemos agregar otras variables predictoras

3. El número de vecinos $k = 7$ fue elegido de manera completamente arbitraria. ¿Existirá una forma de poder elegir este valor de manera óptima?

## Implementación KNN - Tidymodels

Pasemos ahora a implementar el algoritmo KNN en tidymodels. Para esto, primero recordemos los pasos generales a seguir al utilizar esta librería:

1. Dividir los datos
2. Especificación del modelo
3. Pre-procesamiento de los datos y Feature engineering
4. Ajuste del modelo
5. Evaluación del modelo

Realizamos los primeros 3 pasos en el código a continuación.

```{r especificacion modelo}
# 1 - División de los datos
split_info <- rsample::initial_split(
  data = Sacramento,
  prop = 0.75,
  strata = price
)

sacramento_train <- rsample::training(split_info)
sacramento_test  <- rsample::testing(split_info)

## Para seleccionar k, utilizaremos validación cruzada
sacr_cv <- rsample::vfold_cv(sacramento_train, v = 5, strata = price)

# 2 - Especificación del modelo
sacr_model <- parsnip::nearest_neighbor(weight_func = 'rectangular',
                                        neighbors = tune::tune()) %>% 
  set_engine('kknn') %>% 
  set_mode('regression')

# 3 - Especificación de la receta
sacr_recipe <- recipes::recipe(price ~ sqm, data = sacramento_train) %>% 
  step_normalize(all_predictors())
```

Lo anterior es bastante similar a lo que hemos realizado anteriormente, pero sí hay un par de cosas importantes con respecto a la especificación del modelo. Utilizamos `weight_func = 'rectangular'` para realizar knn sin ponderar de acuerdo a los valores, lo cual cambiaremos después. Por otro lado, al colocar `neighbors = tune()` decimos que queremos optimizar ese parámetro, en nuestro caso utilizando validación cruzada.

Ahora si, pasamos a ajustar el modelo buscando el valor óptimo de k

```{r ajuste modelo}
sacr_wf <- 
  workflows::workflow() %>% 
  add_model(sacr_model) %>% 
  add_recipe(sacr_recipe)

sacr_wf

grid_k <- tibble(neighbors = seq(1, 200, by = 3))
sacr_results <- sacr_wf %>% 
  tune::tune_grid(resamples = sacr_cv, grid = grid_k) %>% 
  collect_metrics()

sacr_results
```

Podemos ver en el código a continuación la evolución del RMSE a medida que aumenta el número de vecinos considerados. Además, obtenemos el valor óptimo de $k$.

```{r optimo k}
# Nos fijamos solo en el RMSE
sacr_results_rmse <- sacr_results %>% 
  dplyr::filter(.metric == "rmse")

# Obtenemos el valor de K con el mínimo valor de RMSE
min_k <- sacr_results_rmse %>% 
  filter(mean == min(mean)) %>% 
  .$neighbors
 
# Graficamos
ggplot(sacr_results_rmse, aes(x = neighbors, y = mean)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = min_k, lty = "dashed", col = "red", lwd = 1.5) +
  labs(title = "Evolución RMSE según el número de vecinos",
       x = "Vecinos (K)", y = "RMSE")

print(paste("El valor óptimo de k es", min_k))
```

Vemos entonces que el valor óptimo de k es `r min_k`.

Ya teniendo el valor óptimo de $k$, podemos evaluar nuestro modelo en la base de test que separamos al inicio del análisis.

```{r ajuste final}
# Usamos el valor óptimo de k
sacr_model <- parsnip::nearest_neighbor(weight_func = 'rectangular',
                                        neighbors = min_k) %>% 
  set_engine('kknn') %>% 
  set_mode('regression')

# Ajustamos y obtenemos las medidas de ajuste
sacr_fit <- workflow() %>% 
  add_model(sacr_model) %>% 
  add_recipe(sacr_recipe) %>% 
  fit(data = sacramento_train)

sacr_summary <- sacr_fit %>% 
  predict(sacramento_test) %>% 
  dplyr::bind_cols(sacramento_test) %>% 
  yardstick::metrics(truth = price, estimate = .pred) %>% 
  dplyr::filter(.metric == "rmse")

sacr_summary
```

Obtenemos entonces que, en promedio, se espera que el error de predicción para una nueva observación es de `r round(pull(sacr_summary, .estimate))`.
