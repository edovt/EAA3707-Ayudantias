---
title: "Ayudantía 4 - ML para Negocios"
date: "7 de septiembre del 2022"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "light"
    downcute_theme: "default"
---

```{=html}
<style type="text/css">
body{
  font-size: 15pt;
}
.Wrap {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

```{r setup, include=FALSE, message=FALSE}
library(here)


## Global options
knitr::opts_chunk$set(cache = TRUE)

# Here
here::i_am('Ayudantía 4/Ayudantía 4.Rmd')
```

# Introducción

Bienvenid\@s a la cuarta ayudantía de EAA3707 - Machine Learning para Negocios. En la ayudantía veremos:

1.  Recordatorio modelo de regresión logística
2.  Supuestos del modelo de regresión logística
3.  Algunos tests de hipótesis
3.  Discusión: Regresión logística altamente no balanceada

Antes de comenzar, cargamos las librerías que utilizaremos durante la ayudantía.

```{r librerias, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(car)
library(GGally)
library(ggcorrplot)
library(ResourceSelection)


# Para obtener resultados reproducibles
set.seed(912)
```

La librería `car` será utilizada para realizar diferentes tests para evaluar los supuestos del modelo de regresión logística. Las librerías `GGally` y `ggcorrplot` para realizar gráficos para estudiar colinealidad. Por último, la librería `ResourceSelection` nos permite realizar el test de Hosmer-Lemeshow para la bondad de ajuste de un modelo de regresión logística.

Además, les recuerdo los libros que les recomiendo para aprender de R:

-   **Hands-On Programming with R**: disponible [acá](https://rstudio-education.github.io/hopr/). Útil para recordar cosas básicas de R.

-   **Advanced R**: disponible [acá](https://adv-r.hadley.nz/). Para aprender R avanzado (realmente avanzado), si es que están interesados en entender cómo funciona R por detrás.

-   **R for Data Science**: disponible [acá](https://r4ds.had.co.nz/). Bueno para aprender y aplicar Data Science de manera rápida, además de ser uno de los libros guía para aprender acerca de los paquetes de tidyverse.

-   **RMarkdown Cookbook**: disponible [acá](https://bookdown.org/yihui/rmarkdown-cookbook/). Útil para aprender lo básico de RMarkdown.

-   **RMarkdown: The Definitive Guide**: disponible [acá](https://bookdown.org/yihui/rmarkdown/). Útil para aprender cosas avanzadas de RMarkdown.

-   **Tidy Modeling with R**: disponible [acá](https://www.tmwr.org/). Recomendado para aprender del ecosistema tidymodels. Tiene también un capítulo pequeño de tidyverse con lo justo y necesario.

# Recuerdo del modelo de regresión logística

<center>![Modelo de regresión logística](https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning.png)</center>

Antes de partir con la ayudantía, es importante recordar los conceptos y definiciones importantes del modelo de regresión logística.

Recordemos que en el caso de regresión logística el modelo puede ser escrito como:

$$
\begin{align}
y_i|\mathbf{x}_i &\overset{ind}{\sim} \text{Bernoulli}(\theta_i) \\
g(\theta_i) &= \beta_0 + \beta_1x_{i, 1} + ... + \beta_p x_{i, p} = x^T \beta
\end{align}
$$

donde $g$ es la función logit, esto es,

$$
g(\theta_i) = \text{logit }\theta_i = \log \frac{\theta_i}{1 - \theta_i}
$$

En palabras, lo anterior nos dice que proponemos un modelo en el cual se modela la variable respuesta, $Y$, como realizaciones de una variable aleatoria Bernoulli, esto es, $Y=1$ con probabilidad $\theta$ o $Y=0$ con probabilidad $1 - \theta$, donde el **logit de la media** de la distribución se modela como una función lineal de ciertos predictores o covariables.

--
**Nota**: recuerde que si $Y \sim \text{Bernoulli}(p)$ entonces $E(Y) = p$, por lo que el parámetro corresponde justamente a la media. Esto no necesariamente sucede con cualquier modelo.
--

De manera equivalente, notemos que

$$
g(\theta_i) = \log \frac{\theta_i}{1 - \theta_i} = x_i^T \beta\Leftrightarrow \theta_i = \frac{\exp(x^T\beta)}{1 + \exp(x^T\beta)}
$$

por lo tanto, también podemos decir que la **media** de la distribución es la función logística de la componente lineal $x^T\beta$ (a veces denominada componente sistemática)

--
**Nota**: Como se mencionó la semana pasada, lo único que necesitamos de la función $g$ es que lleve valores de $(0, 1)$ a $(-\infty, \infty)$. La función logit es solo una de ellas, pero está también por ejemplo la función probit o la log-log-complementaria (de hecho, si se fijan la inversa de cualquier función de distribución acumulada cumple lo pedido). Pese a esto, en la práctica el modelo se sigue denominando regresión logística, solo que decimos que cambiamos la **función de enlace**.
--

En la figura a continuación podemos ver las diferencias entre los enlaces logit, probit y log-log-complementario.

```{r distintos enlaces, echo=FALSE, fig.align='center', out.width='80%'}
logistic <- function(x){exp(x)/(exp(x) + 1)}
probit <- function(x){pnorm(x)}
llc <- function(x){1 - exp(-exp(x))}

ggplot(data.frame(x = c(-10, 10)), aes(x = x)) + 
  stat_function(fun = logistic, col = 'red', lwd = 1) +
  stat_function(fun = probit, col = 'blue', lwd = 1) +
  stat_function(fun = llc, col = 'green', lwd = 1) +
  geom_hline(yintercept = c(0, 1), lty = 'dashed') +
  geom_hline(yintercept = 0.5, lty = 'dashed') +
  geom_vline(xintercept = 0, lty = 'dashed') +
  ylim(-0.1, 1.1) +
    labs(title = 'Diferentes funciones de enlace',
         subtitle = 'Rojo - logit, Azul - probit, Verde - llc',
         y = 'p(Y=1|x)')
```

Notamos de lo anterior que tanto el logit como el probit son funciones de enlace simétricas, pero no así el enlace log-log-complementario.

## Definiciones importantes

También es importante recordar la siguiente terminología, que es muy utilizada en el contexto de regresión logística.

Las **chances** u **odds** de un suceso es el cociente entre sus probabilidades de ocurrencia y sus probabilidades de no ocurrencia, esto es,

$$
\text{Odds}(A) = \frac{P(A)}{1 - P(A)}
$$

En el caso de regresión logística con $p$ predictores, el odds de un suceso es el cociente entre sus probabilidades de ocurrencia y sus probabilidades de no ocurrencia, dejando $x$ fijo, esto es

$$
\text{Odds}(Y=1|x) = \frac{\theta(x)}{1 - \theta(x)} = \exp(x^T\beta)
$$

El cambio proporcional en los odds se calcula dividiendo los odds después de un cambio y los odds antes del cambio, lo que se conoce como la **razón de chances** u **odds ratio**.

En el caso de regresión logística, tenemos que si el predictor $j$ aumenta una unidad, entonces

$$
\text{OR}(x_j + 1, x_j) = \frac{\text{Odds}(Y=1|x_j + 1)}{\text{Odds}(Y = 1|x_j)} = \exp(\beta_j)
$$

luego, $\exp(\beta_j)$ se interpreta como el factor por el que cambian las chances cuando el predictor $j$, $x_j$, aumenta en una unidad.

# Supuestos del modelo de regresión logística

Cada modelo que proponemos para describir la realidad se basan, en mayor o menor medida, supuestos que hacemos para simplificar el análisis, sin una gran pérdida de generalidad.

En particular, el modelo de regresión logística también se basa en ciertos supuestos, entre los cuales se encuentran:

1. Linealidad del logit: recordemos que el modelo plantea que
$$
\log \frac{\theta_i}{1 - \theta_i} = x_i^T \beta
$$
2. Independencia de las observaciones
3. No-colinealidad de los predictores: en este caso es importante notar que el problema es grave cuando una varible es **exactamente** o **muy** cercano a una combinación lineal de las otras. En el caso que exista colinealidad no tan perfecta, el problema tiene que ver con las estimaciones del modelo.

Algunos de estos supuestos pueden ser estudiados de manera visual, antes de ajustar los datos, o de manera más formal, lo cual usualmente se realiza **después** de ajustar el modelo. Es importante notar que los chequeos visuales a veces no sirven mucho, ya que no podemos ver en más de 3 dimensiones.

¿Por qué después? porque aprovechamos las propiedades distribucionales de los datos, con lo cual podemos definir estadísticos muestrales. En particular, en los modelos de regresión es común utilizar los **residuos** y aprovecharnos de algunas propiedades distribucionales asintóticas.

**Nota**: podrán notar que el supuesto de homocedasticidad no está presente en el caso de regresión logística.

## Ejemplo práctico

Para estudiar los supuestos anteriores, utilizaremos la misma base de datos de la ayudantía pasada, `credit.data`. Recordemos que esta base contiene información crediticia de diferentes clientes de un banco alemán. En particular, se tiene información de 1000 clientes, con sus diferentes atributos, así como una variable que indica si la persona tiene un mal riesgo crediticio.

**Nota**: recuerden que pueden acceder a toda la información de los datos en el siguiente [enlace](https://archive-beta.ics.uci.edu/ml/datasets/statlog+german+credit+data)

Carguemos entonces la base de datos en el código a continuación. Además, realizamos el preprocesamiento de eliminar algunas variables, cambiar la codificación y agregar nombres informativos a las variables.

```{r carga datos, message=FALSE}
# Cargamos los datos
credit_full <- readr::read_table(
  file = here::here('Ayudantía 4', 'german.data'), 
  col_names = FALSE
)

# Pre-procesamiento de los datos
credit <- 
  credit_full %>% 
  dplyr::select(X2, X3, X5, X13, X15, X16, X21) %>% 
  dplyr::rename(duration = X2,
                credit_history = X3,
                credit_amount = X5,
                age = X13,
                housing = X15,
                n_credits = X16,
                risk = X21) %>%
  dplyr::mutate(risk = risk - 1) %>% # Parte nueva
  dplyr::mutate(housing = dplyr::recode(housing, 
                                        "A151" = 'rent',
                                        "A152" = 'own',
                                        "A153" = 'free'),
                risk = dplyr::recode(risk,
                                     `0` = 'good',
                                     `1` = 'bad')) %>% 
  dplyr::mutate(across(where(is.character), as_factor))

# Vemos cómo nos quedan
dplyr::glimpse(credit)
```

Ya teniendo nuestros datos listos, pasamos a ver el primer supuesto, que es el de la linealidad del logit.

## Linealidad del logit (visual)

Con un predictor $x$ podemos graficar las proporciones muestrales $\sum y_i/n_i$ versus $x_i$, donde $n_i$ es el número de observaciones con valor del predictor $x_i$, lo cual debiese tener forma de S. Equivalentemente, podemos graficar los logitos muestrales versus $x_i$ lo cual debería ser aproximadamente lineal. Pero, ¿cuáles son los problemas con este enfoque?

Primero, si $x$ es continuo, podemos tener que $n_i = 1$ para muchos valores, por lo cual nisiquiera podríamos calcular el logito (obtendríamos $-\infty$ o $\infty$). Una solución sería una pequeña corrección para evitar estos valores problemáticos. Además, si siempre se tiene $n_i = 1$ entonces $y_i/n_i$ tendría solo dos valores posibles, por lo que no podríamos visualizar bien.

Otro problema importante es que esta técnica funcionaría solo en el caso de un predictor (o a lo más dos si usamos gráficos 3D), pero no para los casos generales que son más comunes. Además, puede ser que si vemos gráficos 2D por separado pensemos que la relación no es lineal, pero al verlo de manera conjunta sí existe esta relación.

De todas maneras, veamos dos ejemplo visuales. Primero utilizaremos la variable `Age` que no tiene los problemas de muchos $n_i = 1$:

```{r ejemplo Age, fig.align='center'}
# Podemos contar según la edad, vemos que existe un n_i = 1
age_freqs <- credit %>% 
  dplyr::count(age, name = 'total')
age_freqs
any(age_freqs$total == 1)
sum(age_freqs$total == 1)

# Agrupamos por edad y calculamos las proporciones y logitos muestrales
prop_logit <- credit %>% 
  dplyr::group_by(age, risk, .drop = FALSE) %>% 
  dplyr::count() %>% 
  dplyr::filter(risk == 'bad') %>% 
  dplyr::left_join(age_freqs, by = 'age') %>% 
  dplyr::mutate(prop = n/total)

logitos <- car::logit(prop_logit$prop, adjust = 0.01)
prop_logit$logitos = logitos
```

```{r gráficos age, fig.align='center', echo=FALSE, out.width='80%'}
# Graficamos
## Proporciones
ggplot(prop_logit, mapping = aes(x = age, y = prop)) +
  geom_point() +
  labs(title = 'Análisis visual proporciones muestrales',
       subtitle = 'Usando la variable Age',
       x = 'Edad', y = 'Proporción muestral')

## Logitos
ggplot(prop_logit, mapping = aes(x = age, y = logitos)) +
  geom_point() +
  labs(title = 'Análisis visual linealidad del logito',
       subtitle = 'Usando la variable Age',
       x = 'Edad', y = 'Logito muestral')
```


A través del gráfico podemos ver que al parecer no se cumple el supuesto de linealidad del logito **si sólo consideramos la variable Age**.

Ahora veamos una variable continua, en particular la variable `credit_amount`. En el caso continuo, si no se repiten mucho los valores, es conveniente agrupar los datos en categorías.

```{r ejemplo credit amount, fig.align='center'}
# Podemos contar según el monto del crédito, vemos que en este caso hay 847 n_i = 1
am_freqs <- credit %>% 
  dplyr::count(credit_amount, name = 'total')
am_freqs
sum(am_freqs$total == 1)

# Agrupamos los montos en intervalos
binned <- credit %>% 
  dplyr::mutate(amount_bin = cut(credit_amount, breaks = 15, labels = FALSE)) %>% 
  dplyr::select(credit_amount, amount_bin, risk)

amb_freqs <- binned %>% 
  dplyr::count(amount_bin, name = 'total')

amb_freqs

# Realizamos el conteo y calculamos proporciones y logitos
prop_logit <- binned %>% 
  dplyr::group_by(amount_bin, risk, .drop = FALSE) %>% 
  dplyr::count() %>% 
  dplyr::filter(risk == 'bad') %>% 
  dplyr::left_join(amb_freqs, by = 'amount_bin') %>% 
  dplyr::mutate(prop = n/total)

logitos <- car::logit(prop_logit$prop, adjust = 0.01)
prop_logit$logitos = logitos
```

```{r graficos credit amount, fig.align='center', echo=FALSE, out.width='80%'}
# Graficamos
## Proporciones
ggplot(prop_logit, mapping = aes(x = amount_bin, y = prop)) +
  geom_point() +
  labs(title = 'Análisis visual proporciones muestrales',
       subtitle = 'Usando la variable Credit Amount',
       x = 'Monto del crédito ajustado', y = 'Proporción muestral')

## Logitos
ggplot(prop_logit, mapping = aes(x = amount_bin, y = logitos)) +
  geom_point() +
  labs(title = 'Análisis visual linealidad del logito',
       subtitle = 'Usando la variable Credit Amount',
       x = 'Monto del crédito ajustado', y = 'Logito muestral')
```

Vemos que en este caso sí pareciera existir una pequeña relación lineal **sólo si consideramos el monto del crédito**. 

**Nota**: también podemos ver visualmente los problemas de colinealidad entre las variables, como se explicó anteriormente. Para esto podemos simplemente graficar las variables de a pares (¿qué problema tiene esto?)

```{r gráficos de correlación, fig.align='center', out.width='80%'}
## ggpairs
GGally::ggpairs(credit, columns = c('credit_amount', 'age', 'duration', 'n_credits'))

## ggcorrplot recibe la matriz de correlación
cor_mat <- credit %>% 
  dplyr::select(credit_amount, age, duration, n_credits) %>% 
  as.data.frame() %>% 
  cor()

ggcorrplot::ggcorrplot(cor_mat, lab = TRUE)
```

Es importante recordar que estos chequeos no son formales (estadísticamente). Así, para realizar tests estadísticos debemos ajustar el modelo.

## Ajuste modelo

Repetimos el código de modelamiento usando `tidymodels` de la ayudantía pasada.

```{r modelamiento completo}
# 1 - Dividimos los datos
split_info <- rsample::initial_split(
  data = credit,
  prop = 0.75,
  strata = risk
)

credit_train <- rsample::training(split_info)
credit_test <- rsample::testing(split_info)

# 2 - Especificamos el modelo
logreg_model <- 
  parsnip::logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')

# 3 - Creamos la receta
credit_recipe <- 
  recipes::recipe(risk ~ ., data = credit_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

# 4 - Juntamos todo en un workflow 
logreg_wf <-
  workflows::workflow() %>%
  add_model(logreg_model) %>%
  add_recipe(credit_recipe)

# 5 - Ajustamos el modelo y lo recuperamos
logreg_fit <- logreg_wf %>%
  parsnip::fit(data = credit_train) %>% 
  workflowsets::extract_fit_parsnip() %>% 
  .$fit

# Vemos un resumen del modelo ajustado
summary(logreg_fit)
```

## Test de linealidad del logito

Una vez ajustado el modelo, podemos realizar tests estadísticos formales para estudiar la hipótesis de linealidad del logito. Pero antes, les daré otra forma visual de estudiar una hipótesis **después** de ajustar el modelo.

Para esto, notemos que si $y_i$ corresponde al dato y $\hat{\theta}_i = exp(x^T\hat{\beta})/(1 + \exp(x^T\hat{\beta}))$ al valor ajustado, entonces esperamos que los valores $y_i - \hat{\theta}_i$ sean cercanos a 0 para cada valor de $x^T\hat{\beta}$. Ahora, en este caso tendremos el mismo problema al considerar $x^T\hat{\beta}$, por lo que podemos nuevamente agrupar los datos en intervalos.

```{r grafico residuos, echo=FALSE, fig.align='center', out.width='80%'}
# Recuperamos residuos y valores ajustados
residuos <- residuals(logreg_fit)
ajustados <- fitted(logreg_fit)

aux <- tibble(res = residuos, ajust = ajustados)

ggplot(data = aux, mapping = aes(x = ajust, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 'dashed', col = 'red') +
  labs(title = 'Linealidad del logito', subtitle = 'Sin agrupar')

# Agrupamos nuevamente
aux2 <- aux %>% 
  dplyr::mutate(ajust_bin = cut(ajust, breaks = 20, labels = FALSE)) %>% 
  dplyr::group_by(ajust_bin) %>% 
  dplyr::summarise(res_medio = mean(res))

ggplot(data = aux2, mapping = aes(x = ajust_bin, y = res_medio)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 'dashed', col = 'red') +
  labs(title = 'Linealidad del logito', subtitle = 'Datos agrupados')
```

De manera más formal, podemos realizar el test de Box-Tidwell. Este test tiene por hipótesis nula que la relación entre el logit y cada predictor **continuo** es lineal.

```{r test bt}
aux <- credit %>% 
  dplyr::mutate(risk = as.integer(risk) - 1)

car::boxTidwell(formula = risk ~ duration + credit_amount + age,
                data = aux)
```

Vemos que en este caso el test nos dice que, si consideramos un nivel de significancia $\alpha = 0.05$, ninguna variable **por sí sola** cumple el supuesto de linealidad del logito.

## Independencia de las observaciones (discusión)

## No-colinealidad de los predictores: Factor de inflación de la varianza

En modelos de regresión existen diferentes problemas con respecto a la colinealidad o multicolinealidad de las variables predictoras. En primer lugar, tenemos que si las variables son **perfectamente** multicolineales, entonces ni siquiera podemos calcular los estimadores. Por otro lado, si están **muy** cercanos a ser perfectamente multicolineales, entonces es probable que tengamos problemas numéricos.

Ahora, si el problema no es tan grave, igual pueden existir problemas si tenemos variables altamente colineales, que tienen que ver, más que nada, con que se nos agrandan los errores estándar de cada estimador.

Así, existe una medida que mide multicolinealidad de cada variable con respecto a las otras, denominada el **Factor de Inflación de la Varianza (VIF)**. Se han propuesto diferentes valores de corte, pero un valor $>5$ parece ser prudente.

```{r vif, echo=FALSE}
# Calculamos los VIF
car::vif(logreg_fit)
```

Vemos que en nuestro caso ninguna variable tiene un VIF mayor que 5, por lo que tenemos evidencia que no existe una multicolinealidad pronunciada entre las variables predictoras.

# Algunos tests de bondad de ajuste

## Devianza

```{r resultados modelo}
summary(logreg_fit)
```

## Test de Hosmer-Lemeshow

El test de Hosmer-Lemeshow es un test de bondad de ajuste específico del modelo de regresión logística. De manera simple, lo que hace es dividir las variables predictivas en subgrupos, realizando tests de bondad de ajuste en cada una de estas regiones. El test busca así ver si el modelo está bien calibrado, y que no existan subgrupos en los cuales el ajuste es malo.

```{r test hl}
ResourceSelection::hoslem.test(logreg_fit$y, fitted(logreg_fit))
```


# Discusión: regresión logística altamente no balanceada

- Algoritmo SMOTE (Synthetic Minority Oversampling Technique)

- Algoritmo Near-Miss (undersampling)